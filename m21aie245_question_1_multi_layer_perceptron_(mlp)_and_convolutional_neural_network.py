# -*- coding: utf-8 -*-
"""M21AIE245_Question_1_Multi_layer_perceptron_(MLP)_and_convolutional_neural_network.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nGShqb5uUh60OakGsO08J9LyVwRUt7x-
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
import matplotlib.pyplot as plt
import tensorboard as tb
from torch.utils.tensorboard import SummaryWriter
from sklearn.metrics import classification_report, confusion_matrix

import torch
from torchvision import datasets, transforms

# Define the transformations
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

# Load the train and test sets
trainset = datasets.USPS(root='~/.pytorch/USPS_data/', train=True, download=True, transform=transform)
testset = datasets.USPS(root='~/.pytorch/USPS_data/', train=False, download=True, transform=transform)

# Create the data loaders
train_loader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)
test_loader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)

class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        self.layers = nn.Sequential(
            nn.Linear(28*28, 512),
            nn.ReLU(),
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Linear(10, 10)
        )

    def forward(self, x):
        x = x.view(x.size(0), -2)
        x = self.layers(x)
        return x

# Instantiate the model
model = MLP()
optimizer = torch.optim.Adam(model.parameters(), lr=0.002)

num_epochs =5 

print(model)

for epoch in range(5):
    for i, (images, labels) in enumerate(train_loader):
        # Forward pass
        outputs = model(images)
        loss = nn.CrossEntropyLoss()(outputs, labels)

        # Backward pass and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if (i+1) % 100 == 0:
           print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item()}')

from sklearn.metrics import classification_report, confusion_matrix
import numpy as np

# Compute metrics
model.eval()  # Set the model to evaluation mode
y_test = []
y_pred = []
with torch.no_grad():
    for images, labels in test_loader:
        outputs = model(images)
        _, predicted = torch.max(outputs, 1)
        y_test.extend(labels.numpy().tolist())
        y_pred.extend(predicted.numpy().tolist())

print("Classification Report:")
print(classification_report(y_test, y_pred))

print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

from torch.utils.tensorboard import SummaryWriter
num_epochs =5
writer = SummaryWriter()

for epoch in range(5):
    running_loss = 0.0
    running_corrects = 0
    for i, (images, labels) in enumerate(train_loader):
        # Forward pass
        outputs = model(images)
        loss = nn.CrossEntropyLoss()(outputs, labels)

        # Backward pass and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        running_loss += loss.item() * images.size(0)
        _, preds = torch.max(outputs, 1)
        running_corrects += torch.sum(preds == labels.data)

    epoch_loss = running_loss / len(train_loader.dataset)
    epoch_acc = running_corrects.double() / len(train_loader.dataset)

    writer.add_scalar('training loss', epoch_loss, epoch)
    writer.add_scalar('training accuracy', epoch_acc, epoch)

writer.close()

from torch.utils.tensorboard import SummaryWriter

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.

# %tensorboard --logdir runs